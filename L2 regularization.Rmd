---
title: "L2 regularization"
author: "Roxana Niksefat"
date: '2023-10-26'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this code, L2 regularization is applied to the weights of the hidden layers using the kernel_regularizer argument when adding layers. You can adjust the regularization strength by changing the value (0.01 in this example) to control how much penalty is applied to the weights. L2 regularization helps prevent overfitting by encouraging smaller weight values.   

```{python}
#importing the packages
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
```


```{python}
# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target
```


```{python}
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```


```{python}
# Standardize features (optional but often recommended)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```


```{python}
# Create a sequential model with L2 regularization
model = keras.Sequential()
```


```{python}
# Add an input layer with 4 neurons (input features)
model.add(layers.Input(shape=(4,)))

# Add a hidden layer with 64 neurons and 'relu' activation function with L2 regularization
model.add(layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)))  # Adjust the regularization strength (0.01)

# Add another hidden layer with 64 neurons and 'relu' activation function with L2 regularization
model.add(layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)))  # Adjust the regularization strength (0.01)

# Add an output layer with 3 neurons (for 3 classes) and 'softmax' activation function
model.add(layers.Dense(3, activation='softmax'))
```


```{python}
# Compile the model with L2 regularization
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
```


```{python}
# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=1)
```


```{python}
# Evaluate the model
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
print(classification_report(y_test, y_pred_classes))

```

